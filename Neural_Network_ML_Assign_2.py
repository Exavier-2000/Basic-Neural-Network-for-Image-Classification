# -*- coding: utf-8 -*-
"""Neural_Network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15D4FxwqVhlymokCv6gEK4K7ly3udcTQ3
"""

# from sklearn.datasets import fetch_openml
# from sklearn.model_selection import train_test_split

# # Load MNIST dataset
# mnist = fetch_openml('mnist_784')
# X, y = mnist['data'], mnist['target']

# # Split dataset into train and test sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

# # Verify the shape of the train and test sets
# print("X_train shape:", X_train.shape)
# print("X_test shape:", X_test.shape)
# print("y_train shape:", y_train.shape)
# print("y_test shape:", y_test.shape)

# X_train=X_train.to_numpy()
# X_test=X_test.to_numpy()
# y_train=y_train.to_numpy()
# y_test=y_test.to_numpy()

"""**TanH activation**"""

import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split

# Define the neural network model
class MyModel(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(MyModel, self).__init__()
        self.layers = nn.ModuleList()
        sizes = [input_size] + hidden_sizes + [output_size]
        for i in range(len(sizes) - 1):
            self.layers.append(nn.Linear(sizes[i], sizes[i+1]))
        
    def forward(self, x):
        for layer in self.layers[:-1]:
            x = torch.tanh(layer(x))
        x = self.layers[-1](x)
        return x

# Set hyperparameters
input_size = 784  # MNIST input size
hidden_sizes = [50, 50]  # 2 hidden layers with 50 neurons each
output_size = 10  # MNIST output size
learning_rate = 0.001
num_epochs = 10
batch_size = 128



# Load MNIST dataset
mnist = fetch_openml('mnist_784')
X, y = mnist['data'], mnist['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

# Convert data to pandas DataFrames
X_train_df = pd.DataFrame(X_train)
y_train_df = pd.Series(y_train).astype('category').cat.codes
X_test_df = pd.DataFrame(X_test)
y_test_df = pd.Series(y_test).astype('category').cat.codes

# Create DataLoader for batching
train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_train_df.values).float(), torch.from_numpy(y_train_df.values).long())
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Initialize the model, loss function, and optimizer
model = MyModel(input_size, hidden_sizes, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    for batch_X, batch_y in train_loader:
        # Zero the gradients
        optimizer.zero_grad()
        
        # Forward pass
        outputs = model(batch_X)
        
        # Compute loss
        loss = criterion(outputs, batch_y)
        
        # Backward pass
        loss.backward()
        
        # Update weights
        optimizer.step()
    
    # Print loss for this epoch
    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

# Evaluation
model.eval()
with torch.no_grad():
    X_test_tensor = torch.from_numpy(X_test_df.values).float()
    y_pred = model(X_test_tensor)
    _, predicted = torch.max(y_pred.data, 1)
    total = y_test_df.size
    correct = (predicted == torch.from_numpy(y_test_df.values).long()).sum().item()
    accuracy = correct / total
    print('Test Accuracy: {:.2%}'.format(accuracy))

"""**Sigmoid Activation**"""

# Define the neural network model
class MyModel(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(MyModel, self).__init__()
        self.layers = nn.ModuleList()
        sizes = [input_size] + hidden_sizes + [output_size]
        for i in range(len(sizes) - 1):
            self.layers.append(nn.Linear(sizes[i], sizes[i+1]))
        
    def forward(self, x):
        for layer in self.layers[:-1]:
            x = torch.sigmoid(layer(x))
        x = self.layers[-1](x)
        return x

# Set hyperparameters
input_size = 784  # MNIST input size
hidden_sizes = [50, 50]  # 2 hidden layers with 50 neurons each
output_size = 10  # MNIST output size
learning_rate = 0.001
num_epochs = 10
batch_size = 128



model = MyModel(input_size, hidden_sizes, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

# Evaluation
model.eval()
with torch.no_grad():
    X_test_tensor = torch.from_numpy(X_test_df.values).float()
    y_pred = model(X_test_tensor)
    _, predicted = torch.max(y_pred.data, 1)
    total = y_test_df.size
    correct = (predicted == torch.from_numpy(y_test_df.values).long()).sum().item()
    accuracy = correct / total
    print('Test Accuracy: {:.2%}'.format(accuracy))

"""**ReLU Activation Function**"""

# Define the neural network model
class MyModel(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(MyModel, self).__init__()
        self.layers = nn.ModuleList()
        sizes = [input_size] + hidden_sizes + [output_size]
        for i in range(len(sizes) - 1):
            self.layers.append(nn.Linear(sizes[i], sizes[i+1]))
        
    def forward(self, x):
        for layer in self.layers[:-1]:
            x = torch.relu(layer(x))
        x = self.layers[-1](x)
        return x

# Set hyperparameters
input_size = 784  # MNIST input size
hidden_sizes = [50, 50]  # 2 hidden layers with 50 neurons each
output_size = 10  # MNIST output size
learning_rate = 0.001
num_epochs = 10
batch_size = 128



model = MyModel(input_size, hidden_sizes, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

# Evaluation
model.eval()
with torch.no_grad():
    X_test_tensor = torch.from_numpy(X_test_df.values).float()
    y_pred = model(X_test_tensor)
    _, predicted = torch.max(y_pred.data, 1)
    total = y_test_df.size
    correct = (predicted == torch.from_numpy(y_test_df.values).long()).sum().item()
    accuracy = correct / total
    print('Test Accuracy: {:.2%}'.format(accuracy))

"""**3 hidden layer + relu**"""

# Define the neural network model
class MyModel(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(MyModel, self).__init__()
        self.layers = nn.ModuleList()
        sizes = [input_size] + hidden_sizes + [output_size]
        for i in range(len(sizes) - 1):
            self.layers.append(nn.Linear(sizes[i], sizes[i+1]))
        
    def forward(self, x):
        for layer in self.layers[:-1]:
            x = torch.relu(layer(x))
        x = self.layers[-1](x)
        return x

# Set hyperparameters
input_size = 784  # MNIST input size
hidden_sizes = [50, 50,50]  # 2 hidden layers with 50 neurons each
output_size = 10  # MNIST output size
learning_rate = 0.001
num_epochs = 10
batch_size = 128



model = MyModel(input_size, hidden_sizes, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

# Evaluation
model.eval()
with torch.no_grad():
    X_test_tensor = torch.from_numpy(X_test_df.values).float()
    y_pred = model(X_test_tensor)
    _, predicted = torch.max(y_pred.data, 1)
    total = y_test_df.size
    correct = (predicted == torch.from_numpy(y_test_df.values).long()).sum().item()
    accuracy = correct / total
    print('Test Accuracy: {:.2%}'.format(accuracy))

"""**3 hidden + sigmoid**"""

# Define the neural network model
class MyModel(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(MyModel, self).__init__()
        self.layers = nn.ModuleList()
        sizes = [input_size] + hidden_sizes + [output_size]
        for i in range(len(sizes) - 1):
            self.layers.append(nn.Linear(sizes[i], sizes[i+1]))
        
    def forward(self, x):
        for layer in self.layers[:-1]:
            x = torch.sigmoid(layer(x))
        x = self.layers[-1](x)
        return x

# Set hyperparameters
input_size = 784  # MNIST input size
hidden_sizes = [50, 50,50]  # 2 hidden layers with 50 neurons each
output_size = 10  # MNIST output size
learning_rate = 0.001
num_epochs = 10
batch_size = 128



model = MyModel(input_size, hidden_sizes, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

# Evaluation
model.eval()
with torch.no_grad():
    X_test_tensor = torch.from_numpy(X_test_df.values).float()
    y_pred = model(X_test_tensor)
    _, predicted = torch.max(y_pred.data, 1)
    total = y_test_df.size
    correct = (predicted == torch.from_numpy(y_test_df.values).long()).sum().item()
    accuracy = correct / total
    print('Test Accuracy: {:.2%}'.format(accuracy))

"""**3 hidden + tanh**"""

# Define the neural network model
class MyModel(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(MyModel, self).__init__()
        self.layers = nn.ModuleList()
        sizes = [input_size] + hidden_sizes + [output_size]
        for i in range(len(sizes) - 1):
            self.layers.append(nn.Linear(sizes[i], sizes[i+1]))
        
    def forward(self, x):
        for layer in self.layers[:-1]:
            x = torch.tanh(layer(x))
        x = self.layers[-1](x)
        return x

# Set hyperparameters
input_size = 784  # MNIST input size
hidden_sizes = [50, 50,50]  # 2 hidden layers with 50 neurons each
output_size = 10  # MNIST output size
learning_rate = 0.001
num_epochs = 10
batch_size = 128



model = MyModel(input_size, hidden_sizes, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

# Evaluation
model.eval()
with torch.no_grad():
    X_test_tensor = torch.from_numpy(X_test_df.values).float()
    y_pred = model(X_test_tensor)
    _, predicted = torch.max(y_pred.data, 1)
    total = y_test_df.size
    correct = (predicted == torch.from_numpy(y_test_df.values).long()).sum().item()
    accuracy = correct / total
    print('Test Accuracy: {:.2%}'.format(accuracy))

"""**3 hidden with 75,74,1 layers + tanh**"""

# Define the neural network model
class MyModel(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(MyModel, self).__init__()
        self.layers = nn.ModuleList()
        sizes = [input_size] + hidden_sizes + [output_size]
        for i in range(len(sizes) - 1):
            self.layers.append(nn.Linear(sizes[i], sizes[i+1]))
        
    def forward(self, x):
        for layer in self.layers[:-1]:
            x = torch.relu(layer(x))
        x = self.layers[-1](x)
        return x

# Set hyperparameters
input_size = 784  # MNIST input size
hidden_sizes = [74, 75,1]  # 2 hidden layers with 50 neurons each
output_size = 10  # MNIST output size
learning_rate = 0.001
num_epochs = 10
batch_size = 128



model = MyModel(input_size, hidden_sizes, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

# Evaluation
model.eval()
with torch.no_grad():
    X_test_tensor = torch.from_numpy(X_test_df.values).float()
    y_pred = model(X_test_tensor)
    _, predicted = torch.max(y_pred.data, 1)
    total = y_test_df.size
    correct = (predicted == torch.from_numpy(y_test_df.values).long()).sum().item()
    accuracy = correct / total
    print('Test Accuracy: {:.2%}'.format(accuracy))

"""**3 hidden with 75,74,1 layers + sigmoid**"""

# Define the neural network model
class MyModel(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(MyModel, self).__init__()
        self.layers = nn.ModuleList()
        sizes = [input_size] + hidden_sizes + [output_size]
        for i in range(len(sizes) - 1):
            self.layers.append(nn.Linear(sizes[i], sizes[i+1]))
        
    def forward(self, x):
        for layer in self.layers[:-1]:
            x = torch.sigmoid(layer(x))
        x = self.layers[-1](x)
        return x

# Set hyperparameters
input_size = 784  # MNIST input size
hidden_sizes = [74, 75,1]  # 2 hidden layers with 50 neurons each
output_size = 10  # MNIST output size
learning_rate = 0.001
num_epochs = 10
batch_size = 128



model = MyModel(input_size, hidden_sizes, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

# Evaluation
model.eval()
with torch.no_grad():
    X_test_tensor = torch.from_numpy(X_test_df.values).float()
    y_pred = model(X_test_tensor)
    _, predicted = torch.max(y_pred.data, 1)
    total = y_test_df.size
    correct = (predicted == torch.from_numpy(y_test_df.values).long()).sum().item()
    accuracy = correct / total
    print('Test Accuracy: {:.2%}'.format(accuracy))

"""**3 hidden with 75,74,1 layers + relu**"""

# Define the neural network model
class MyModel(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(MyModel, self).__init__()
        self.layers = nn.ModuleList()
        sizes = [input_size] + hidden_sizes + [output_size]
        for i in range(len(sizes) - 1):
            self.layers.append(nn.Linear(sizes[i], sizes[i+1]))
        
    def forward(self, x):
        for layer in self.layers[:-1]:
            x = torch.relu(layer(x))
        x = self.layers[-1](x)
        return x

# Set hyperparameters
input_size = 784  # MNIST input size
hidden_sizes = [74, 75,1]  # 2 hidden layers with 50 neurons each
output_size = 10  # MNIST output size
learning_rate = 0.001
num_epochs = 10
batch_size = 128



model = MyModel(input_size, hidden_sizes, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

# Evaluation
model.eval()
with torch.no_grad():
    X_test_tensor = torch.from_numpy(X_test_df.values).float()
    y_pred = model(X_test_tensor)
    _, predicted = torch.max(y_pred.data, 1)
    total = y_test_df.size
    correct = (predicted == torch.from_numpy(y_test_df.values).long()).sum().item()
    accuracy = correct / total
    print('Test Accuracy: {:.2%}'.format(accuracy))

"""**Different train test split with 50-50 and 74,75,1 hidden layer**"""

# Define the neural network model
class MyModel(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(MyModel, self).__init__()
        self.layers = nn.ModuleList()
        sizes = [input_size] + hidden_sizes + [output_size]
        for i in range(len(sizes) - 1):
            self.layers.append(nn.Linear(sizes[i], sizes[i+1]))
        
    def forward(self, x):
        for layer in self.layers[:-1]:
            x = torch.relu(layer(x))
        x = self.layers[-1](x)
        return x

# Set hyperparameters
input_size = 784  # MNIST input size
hidden_sizes = [74, 75,1]  # 2 hidden layers with 50 neurons each
output_size = 10  # MNIST output size
learning_rate = 0.001
num_epochs = 10
batch_size = 128

# Load MNIST dataset
mnist = fetch_openml('mnist_784')
X, y = mnist['data'], mnist['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)

# Convert data to pandas DataFrames
X_train_df = pd.DataFrame(X_train)
y_train_df = pd.Series(y_train).astype('category').cat.codes
X_test_df = pd.DataFrame(X_test)
y_test_df = pd.Series(y_test).astype('category').cat.codes

# Create DataLoader for batching
train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_train_df.values).float(), torch.from_numpy(y_train_df.values).long())
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

model = MyModel(input_size, hidden_sizes, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

# Evaluation
model.eval()
with torch.no_grad():
    X_test_tensor = torch.from_numpy(X_test_df.values).float()
    y_pred = model(X_test_tensor)
    _, predicted = torch.max(y_pred.data, 1)
    total = y_test_df.size
    correct = (predicted == torch.from_numpy(y_test_df.values).long()).sum().item()
    accuracy = correct / total
    print('Test Accuracy: {:.2%}'.format(accuracy))

"""**With 80-20 split and equal split of neurons 50-50-50 and relu**"""

# Define the neural network model
class MyModel(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(MyModel, self).__init__()
        self.layers = nn.ModuleList()
        sizes = [input_size] + hidden_sizes + [output_size]
        for i in range(len(sizes) - 1):
            self.layers.append(nn.Linear(sizes[i], sizes[i+1]))
        
    def forward(self, x):
        for layer in self.layers[:-1]:
            x = torch.relu(layer(x))
        x = self.layers[-1](x)
        return x

# Set hyperparameters
input_size = 784  # MNIST input size
hidden_sizes = [50,50,50]  # 2 hidden layers with 50 neurons each
output_size = 10  # MNIST output size
learning_rate = 0.001
num_epochs = 10
batch_size = 128

# Load MNIST dataset
mnist = fetch_openml('mnist_784')
X, y = mnist['data'], mnist['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)

# Convert data to pandas DataFrames
X_train_df = pd.DataFrame(X_train)
y_train_df = pd.Series(y_train).astype('category').cat.codes
X_test_df = pd.DataFrame(X_test)
y_test_df = pd.Series(y_test).astype('category').cat.codes

# Create DataLoader for batching
train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_train_df.values).float(), torch.from_numpy(y_train_df.values).long())
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

model = MyModel(input_size, hidden_sizes, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

# Evaluation
model.eval()
with torch.no_grad():
    X_test_tensor = torch.from_numpy(X_test_df.values).float()
    y_pred = model(X_test_tensor)
    _, predicted = torch.max(y_pred.data, 1)
    total = y_test_df.size
    correct = (predicted == torch.from_numpy(y_test_df.values).long()).sum().item()
    accuracy = correct / total
    print('Test Accuracy: {:.2%}'.format(accuracy))

"""**Hidden layers with 74+75+1 and with 80-20 split+Sigmoid**"""

# Define the neural network model
class MyModel(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(MyModel, self).__init__()
        self.layers = nn.ModuleList()
        sizes = [input_size] + hidden_sizes + [output_size]
        for i in range(len(sizes) - 1):
            self.layers.append(nn.Linear(sizes[i], sizes[i+1]))
        
    def forward(self, x):
        for layer in self.layers[:-1]:
            x = torch.sigmoid(layer(x))
        x = self.layers[-1](x)
        return x

# Set hyperparameters
input_size = 784  # MNIST input size
hidden_sizes = [74, 75,1]  # 2 hidden layers with 50 neurons each
output_size = 10  # MNIST output size
learning_rate = 0.001
num_epochs = 10
batch_size = 128



model = MyModel(input_size, hidden_sizes, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

# Evaluation
model.eval()
with torch.no_grad():
    X_test_tensor = torch.from_numpy(X_test_df.values).float()
    y_pred = model(X_test_tensor)
    _, predicted = torch.max(y_pred.data, 1)
    total = y_test_df.size
    correct = (predicted == torch.from_numpy(y_test_df.values).long()).sum().item()
    accuracy = correct / total
    print('Test Accuracy: {:.2%}'.format(accuracy))

"""**2 hidden layers with 80,20+ Sigmoid**"""

# Define the neural network model
class MyModel(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(MyModel, self).__init__()
        self.layers = nn.ModuleList()
        sizes = [input_size] + hidden_sizes + [output_size]
        for i in range(len(sizes) - 1):
            self.layers.append(nn.Linear(sizes[i], sizes[i+1]))
        
    def forward(self, x):
        for layer in self.layers[:-1]:
            x = torch.sigmoid(layer(x))
        x = self.layers[-1](x)
        return x

# Set hyperparameters
input_size = 784  # MNIST input size
hidden_sizes = [50,50]  # 2 hidden layers with 50 neurons each
output_size = 10  # MNIST output size
learning_rate = 0.001
num_epochs = 10
batch_size = 128



model = MyModel(input_size, hidden_sizes, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

# Evaluation
model.eval()
with torch.no_grad():
    X_test_tensor = torch.from_numpy(X_test_df.values).float()
    y_pred = model(X_test_tensor)
    _, predicted = torch.max(y_pred.data, 1)
    total = y_test_df.size
    correct = (predicted == torch.from_numpy(y_test_df.values).long()).sum().item()
    accuracy = correct / total
    print('Test Accuracy: {:.2%}'.format(accuracy))

"""**2 hidden layers with 80,20+ tanh**"""

# Define the neural network model
class MyModel(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(MyModel, self).__init__()
        self.layers = nn.ModuleList()
        sizes = [input_size] + hidden_sizes + [output_size]
        for i in range(len(sizes) - 1):
            self.layers.append(nn.Linear(sizes[i], sizes[i+1]))
        
    def forward(self, x):
        for layer in self.layers[:-1]:
            x = torch.tanh(layer(x))
        x = self.layers[-1](x)
        return x

# Set hyperparameters
input_size = 784  # MNIST input size
hidden_sizes = [50,50]  # 2 hidden layers with 50 neurons each
output_size = 10  # MNIST output size
learning_rate = 0.001
num_epochs = 10
batch_size = 128



model = MyModel(input_size, hidden_sizes, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

# Evaluation
model.eval()
with torch.no_grad():
    X_test_tensor = torch.from_numpy(X_test_df.values).float()
    y_pred = model(X_test_tensor)
    _, predicted = torch.max(y_pred.data, 1)
    total = y_test_df.size
    correct = (predicted == torch.from_numpy(y_test_df.values).long()).sum().item()
    accuracy = correct / total
    print('Test Accuracy: {:.2%}'.format(accuracy))

"""**2 hidden layers with 80,20+ relu**"""

# Define the neural network model
class MyModel(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(MyModel, self).__init__()
        self.layers = nn.ModuleList()
        sizes = [input_size] + hidden_sizes + [output_size]
        for i in range(len(sizes) - 1):
            self.layers.append(nn.Linear(sizes[i], sizes[i+1]))
        
    def forward(self, x):
        for layer in self.layers[:-1]:
            x = torch.relu(layer(x))
        x = self.layers[-1](x)
        return x

# Set hyperparameters
input_size = 784  # MNIST input size
hidden_sizes = [50,50]  # 2 hidden layers with 50 neurons each
output_size = 10  # MNIST output size
learning_rate = 0.001
num_epochs = 10
batch_size = 128



model = MyModel(input_size, hidden_sizes, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

# Evaluation
model.eval()
with torch.no_grad():
    X_test_tensor = torch.from_numpy(X_test_df.values).float()
    y_pred = model(X_test_tensor)
    _, predicted = torch.max(y_pred.data, 1)
    total = y_test_df.size
    correct = (predicted == torch.from_numpy(y_test_df.values).long()).sum().item()
    accuracy = correct / total
    print('Test Accuracy: {:.2%}'.format(accuracy))

"""**2 hidden layers with 99+1 and relu and 80-20 split**"""

# Define the neural network model
class MyModel(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(MyModel, self).__init__()
        self.layers = nn.ModuleList()
        sizes = [input_size] + hidden_sizes + [output_size]
        for i in range(len(sizes) - 1):
            self.layers.append(nn.Linear(sizes[i], sizes[i+1]))
        
    def forward(self, x):
        for layer in self.layers[:-1]:
            x = torch.relu(layer(x))
        x = self.layers[-1](x)
        return x

# Set hyperparameters
input_size = 784  # MNIST input size
hidden_sizes = [99,1]  # 2 hidden layers with 50 neurons each
output_size = 10  # MNIST output size
learning_rate = 0.001
num_epochs = 10
batch_size = 128



model = MyModel(input_size, hidden_sizes, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

# Evaluation
model.eval()
with torch.no_grad():
    X_test_tensor = torch.from_numpy(X_test_df.values).float()
    y_pred = model(X_test_tensor)
    _, predicted = torch.max(y_pred.data, 1)
    total = y_test_df.size
    correct = (predicted == torch.from_numpy(y_test_df.values).long()).sum().item()
    accuracy = correct / total
    print('Test Accuracy: {:.2%}'.format(accuracy))